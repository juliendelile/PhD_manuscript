
 
 
 
 
<div id='6_'>
<h1>6. Reconstruction of the Zebrafish early development  </h1>
 


<figure id=''>
	<img src="../../images/Reconstruction/bioemergences/c_elegans_lineage_kipreos_2005.png" width="900">
	<figcaption>Cell lineage from the <i>Caenorhabditis elegans</i> embryonics and larval stages from Kipreos, E.T., 2005. C. elegans cell cycles: invariance and stem cell divisions. Nature reviews Molecular cell biology, 6(10), pp.766–776. <a name='Kipreos:2005im' class='ref'></a></figcaption>
</figure>



<p>
	<strong>Objective</strong>: Reconstruction of the complete cell lineage tree of the Zebrafish early development. The lineage tree is augmented by the spatial coordinates and the shape of the cells (3D+time digital embryo) and by the quantification of various gene expression of the cells (Atlas of genetic expression).
</p>


<p>
	The reconstruction task is a very tedious one and semi or complete automation is the only solution to tackle such an enormous lineage tree.
</p>

<p>
	In this section, we will introduce the past and current strategies originated by the European project Embryomics and Bioemergences and pursued the Bioemergences team. This section is divided as the following:
</p>

<ul>
	<li>1. Embryo Preparation and Acquisition</li>
	<li>2. Cell lineage reconstruction workflow</li>
	<ul>
		<li>1. Workflow</li>
		<li>2. Visualization / Correction</li>
		<li>3. Validation</li>
	</ul>
	<li>3. Post-processing / Higher level reconsruction / Exploitation</li>
	<li>4. 4D atlas of gene expression</li>
	<li>5. <i>In toto</i> cell lineage</li>
</ul>



Discussion
->Prototype

<p>
	<strong>Principal Cell Lineage Reconstruction workflow</strong>:
</p>

<ul>
	<li>Filtering</li>
	<li>Cell Identification</li>
	<li>Shape Segmentation</li>
	<li>Cell Tracking</li>
</ul>






	<li>Interactive Visualization</li>
	<li>Validation</li>



<p>
	<strong>Various challenges</strong>: in addition to the challenges to design the aforementioned reconstruction strategy, various challenges arises from the limitation of the current imaging setups. 
</p>



<p>
	Workflow inputs: raw data
</p>


<ul>
	<li>The workflow is efficient if the spatio-temporal resolution of the raw data produced by the microscope device satisfies some required signal properties.</li>
	<ul>
		<li>spatially, the signal-to-noise ratio must be limited and the local resolution of the 
		<li>temporally, the difference between two images of the same region must be small enough as the image must not move too much.</li>
	</ul>
</ul>

<p>
	Unfortunately, these two requirements are antagonists and a trade-off must be found
</p>




Solution: 

<p>
	Most of the improvement must be applied in the imaging technique area but, given their limitations, algorithmic bandages must be designed to overcome them.
</p>



<i>in toto</i>

error-less: automated vs manual (introduire application thierry),

channel limitation



The prototype








<p>
	review of embryogenesis reconstruction :
</p>


<p>
	Keller, P.J. et al., 2008. Reconstruction of zebrafish early embryonic development by scanned light sheet microscopy. Science, 322(5904), pp.1065–1069. <a name='Keller:2008km' class='fig'></a>
</p>


<p>
	Giurumescu, C.A. et al., 2012. Quantitative semi-automated analysis of morphogenesis with single-cell resolution in complex embryos. Development, 139(22), pp.4271–4279. <a name='Giurumescu:2012ej' class='fig'></a>
</p>




<p>
	example reconstruction:
</p>


<p>
	Xiong, Y. & Iglesias, P.A., 2010. Tools for analyzing cell shape changes during chemotaxis. Integrative Biology, 2(11-12), pp.561–567. <a name='Xiong:2010ck' class='ref'></a>
</p>





</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_1_'>
<h2>6.1. Production of experimental data  </h2>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_1_1_'>
<h2>6.1.1. Microscopy  </h2>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_1_2_'>
<h2>6.1.2. RNAinjection  </h2>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_1_3_'>
<h2>6.1.3. Mutants  </h2>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_1_4_'>
<h2>6.1.4. Atlas  </h2>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_2_'>
<h2>6.2. Reconstruction workflow  </h2>
 
 
	<p>
		review -> Khairy, K. & Keller, P.J., 2011. Reconstructing embryonic development P. M. Kulesa, M. E. Dickinson, & A.-K. Hadjantonakis, eds. genesis, 49(7), pp.488–513. <a name='Khairy:2011ht' class='ref'></a>
	</p>



</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_2_1_'>
<h2>6.2.1. Bioemergences existing workflow  </h2>
 

<figure id=''>
	<img src="../../images/Reconstruction/bioemergences/FigureWorkflow/Workflow2.png" width="600">
	<figcaption>Workflow of reconstruction of the cell lineage</figcaption>
</figure>





 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_2_2_'>
<h2>6.2.2. New modules  </h2>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_2_2_1_'>
<h2>Methodology</h2>
 
 





	<figure id='washington_workflow'>
		<img src="../../images/Reconstruction/washington/workflow.png" width="900">
		<figcaption><i>in toto</i> displacement field reconstruction workflow</figcaption>
	</figure>

<p>
	The bi-photonic microscopy devices do not allow the capture of the entire zebrafish embryo because of the optical loss of signal in the depth of the tissue.
</p>

<p>
	One solution is to take advantage of the spherical shape of the embryo during the early gastrulation and rotate it to capture multiple views and thus cover the all volume without major deformation. This solution has a pitfall, the imaging time lasts longer as consecutive angles of view have to capture superposed region of the embryo. Fortunately, a recent microscopic device, the Digital Scanning Light Microscope (DSLM), increases the speed of light-scanning thanks to its ability to illuminate and record the signal emitted by the embryo plane by plane and not only point by point. Faster recording means the possibility of make temporal comparison of 3D volume to extract the dynamics of the reconstructed cell trajectories evoked in the previous section.
</p>

<p>
	However, the quality of the signal is different that the one generated by the bi-photon illuminated device used in the previous section so the reconstruction strategies had to be adapted to extract the reconstructed dynamics. We decide to leave the reconstruction of the <i>in toto</i> lineage tree for further investigation and instead create the more accessible reconstruction of the <i>in toto</i> deformation field of the embryo. The workflow is presented in this section.
</p>


<p>
	Resumé des étapes XXXX à faire:
</p>



	<figure id='washington_5angles_before'>
	<img src="../../images/Reconstruction/washington/5angles_before.png" width="900">
	<figcaption></figcaption>
</figure>




</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_2_2_2_'>
<h2>Manual registration</h2>
 
 


	
	<figure id='washington_workflow_manual'>
		<img src="../../images/Reconstruction/washington/workflow_manual.png" width="600">
		<figcaption></figcaption>
	</figure>




	<p>
		The DSLM allows the rapid imaging of embryos and, coupled with a rotating device, is able to record multiple views of the same embryos. In the following, we will present an example of multi-view with five angles of view. 
	</p>


	<p>
		Describe the DSLM
	</p>


	<p>
		In this section 6.2 ???XXXX, we will use the following conventions for the axes of the each views. The axis from the camera to embryo is the z-axis or depth, the horizontal axis of the light plane is the x-axis and the vertical axis of the light plane is the y-axis.
	</p>


	<figure id='washington_SPIM'>
		<img src="../../images/Reconstruction/washington/SPIM.png" width="900">
		<figcaption>SPIM set up.</figcaption>
	</figure>





	<p>
		Reconstructing an <i>in toto</i> view of the embryo require the spatial fusion of the five views in a reference frame. Even if sophisticated method of non-rigid spatial registration based on raw image processing are currently developed (curvelet-based ??? XXXX josé ???), we decide to take advantage of the knowledge of the geometrical motion of the rotating plate of the device to geometrically infer a rigid spatial registration. Moreover, we assume that the spatial registration parameters will remained unchanged for each time step on the imaging process as the embryo does not move in its agarose chamber and the rotating plate exerts precise and controlled displacement. So the parameters are computed at a single time step and will be applied to every time steps.
	</p>




	<figure id='washington_manual_spim'>
		<img src="../../images/Reconstruction/washington/manual_spim.png" width="900">
		<figcaption></figcaption>
	</figure>


	<p>
		The embryo is mounted into a cylindrical agarose tube. The tube is fixed on a table which can rotate around the cylinder central axis and translate it. As the embryo never passes exactly through the central axis of the cylinder, the software which control the motion of the table as to be calibrated before the time-lapse starts. The calibration phase is equivalent to the following process: the operator is asked to center the embryo in the light plane. Once he validates the position, the software performs a 100 degree rotation and the operator is again asked to center the embryo in the light plane. From the translation vector that operator has determined to re-center the embryo, the software computes the relative position of the embryo to the cylinder rotation center. However, the estimation of the center of the embryo is not really precisely done by the operator. First, he only sees a projection along the z-axis, and as the embryo has been rotated, he does not have the same aspect on the screen. The computed relative position is only an approximation of the real relative position.
	</p>



	<figure id='washington_manual_registration_selection'>
		<img src="../../images/Reconstruction/washington/manual_registration_selection.png" width="900">
		<figcaption></figcaption>
	</figure>

	<p>
		We built an interactive graphical interface to manually correct the estimation error, and determine the optimal relative position. The two parameters controlled by the user of the interface are the coordinates of the correction vector. As the user moves them, he can directly observed the computed motion of the 5 views and thus determine the optimal correction vector. For this task, the margin nuclei which are separated from the mass of the deep cells offer useful landmarks to allow a precise determination of these two parameters (figure XXXX).
	</p>


	<figure id='washington_manual_registration_final'>
		<img src="../../images/Reconstruction/washington/manual_registration_final.png" width="900">
		<figcaption></figcaption>
	</figure>

	<p>
		The 5 views are now registrated and the time lapse registration may be performed. However, as it can be seen on figure XXX, the optical deformation induced by the perturbation of the light path by the embryonic tissue creates a very unsatisfying result, the following module will aim at cleaning the deformed part of each views.
	</p>



	<figure id='washington_5angles_after'>
		<img src="../../images/Reconstruction/washington/5angles_after.png" width="900">
		<figcaption></figcaption>
	</figure>





</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_2_2_3_'>
<h2>Voxel quality evaluation</h2>
 

	<figure id='washington_workflow_vqe'>
		<img src="../../images/Reconstruction/washington/workflow_vqe.png" width="600">
		<figcaption></figcaption>
	</figure>
 
	
	<p>
		This module is independent form the previous one and it aims at determining, on each view of each time step, which spatial region of the volume has the better signal quality. The effective goal of this module is to attribute a score to each voxel of the volume, score which expresses the quality of signal of this voxel.
	</p>

	<p>
		The assumption supporting the voxel quality evaluation is based on the observation that the more the light path pass through the embryo, the more the signal is degraded. The light path through the embryo is decomposed in two sub-paths: the path of light from the laser to excited molecule, and the path of light emitted by the molecule to the camera. As the relevant part of the light path is inside the embryo, we perform a geometrical computation to determine the length of light path for each voxel of the volume from the physical border of the embryo. The fist part of this module focuses on the computation of this border and the second on the computation of the length. It may be summarized as the following:
	</p>

	<ul>
		<li>1. To illuminate the low intensity background noise, the average intensity of the voxel in the volume is computed and used to determine the isosurface of the nuclei shapes (vtkContourFilter) (figure XXXX a).  </li>
		<li>2. After a random decimation of the triangles to lower the number of their vertices (vtkDecimatePro), large enough metaballs centered on these vertices are created to obtain a non-empty surface of the embryo (vtkGaussianSplatter then vtkContourFilter). A binary stencil of the image is extracted to store the shape of the embryo (vtkImageStencil).</li>
		<li>3. As some undesired signal may remain out of the embryo shape, we suppose that the closed region which has the larger volume correspond to the embryo so we remove the smaller ones (figure XXXX b).</li>
		<li>4. The metaballs create a embryo border volume which is larger than desired so we erode the volume by the radius of the metaball to get the physical border of the embryo. The volume is stored as a binary data with value 1 (0) for voxel inside (outside) the border (figure XXXX c).</li>
		<li>5. As each view is oriented with the z-axis along the camera path and the x-axis along the illumination plane, it is straightforward to compute both distances from the border along the x and z axes as the number of 1 along both axes (figure XXXX d, e). The voxels which are out of the border get an eliminatory score as they can not be used for the registration task (figure XXXX f).</li>
		<li>6. We also eliminate the voxels which are situated after a cutoff value in both direction as they belong to part of the image that is too degraded to be used (the cutoff value is about 200 microns) (figure XXX g, h, i).</li>
	</ul>
	







	<figure id='washington_vqe'>
		<img src="../../images/Reconstruction/washington/vqe.png" width="900">
		<figcaption></figcaption>
	</figure>





</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_2_2_4_'>
<h2>Blending function  </h2>
 



	<figure id='washington_workflow_blend'>
		<img src="../../images/Reconstruction/washington/workflow_blend.png" width="600">
		<figcaption></figcaption>
	</figure>

	

	<p>
		The objective of this task is to use both the manual registration and the voxel quality evaluation module to build a blending function which will select which part of which view is used in the <i>in toto</i> registrated volume. The function is represented as a  3D integer volume whom each voxel correspond to the id of the more relevant angle of view according to its quality evaluation.
	</p>

	<p>
		With the coordinates of center of rotation of the microscope table, we are now able to set all 5 views into a common reference volume (figure XXXX). We scan each voxel of this reference volume and check in each projected angle of view if it belongs to the inside of the embryo border evaluated in the previous section. If it belongs :
	</p>

	<ul>
		<li>to none of them, a 0 value is attributed to the reference voxel,</li>
		<li>to an evaluated part of a single angle of view, the id of this angle of view is attributed to the reference voxel (from 1 to 5),</li>
		<li>to evaluated parts of multiple angles of view, we pick the id of the angle of view which provide the higher voxel quality evaluation at this position and attribute it to the reference voxel. </li>
	</ul>

	<p>
		This allows the production of the integer mask shown in figure XXX. This mask is used to extract the voxel information for the relevant angle of view. Instead of using the maximum voxel quality evaluation, we could also use these scores to compute an weighted average of each angle.
	</p>

	<p>
		An exemple of application is exposed in figure XXX with the direct selection of the pixel intensity, it provide a first <i>in toto</i> registration of the embryo. However, the objective of this workflow is to apply the blending function to the deformation field.
	</p>



	<figure id='Reconstruction_5angles_3Dvolume'>
		<img src="../../images/Reconstruction/5angles_3Dvolume.png" width="450">
		<img src="../../images/Reconstruction/5angles_2D_scan.png" width="450">
		<figcaption></figcaption>
	</figure>
	


	
	<figure id='washington_blendingfunction'>
		<img src="../../images/Reconstruction/washington/blendingfunction.png" width="900">
		<figcaption></figcaption>
	</figure>




	<figure id='washington_blendingfunction_raw'>
		<img src="../../images/Reconstruction/washington/blendingfunction_raw.png" width="900">
		<figcaption></figcaption>
	</figure>




 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_2_2_5_'>
<h2>Deformation fields</h2>
 


vtkGaussianSplatter


	<figure id='washington_workflow_deffield'>
		<img src="../../images/Reconstruction/washington/workflow_deffield.png" width="600">
		<figcaption></figcaption>
	</figure>


	<p>
		The objective of the module is to reconstruct the spatial deformation that relate two 3D volume at consecutive time step. Each volume is one of the five angles of view produced by the DLSM. We call A and B two generic consecutive 3D volumes. A and B must have the same dimension.
	</p>

	<p>
		The notion of image deformation is close to the notion of optical flow as it aims at computing the image motion between consecutive images. We use a non-rigid registration method called &quot;Demons Registration&quot;, it acts as if voxel-sized &quot;demons&quot; were were pulling and push the voxel of volume B along the local gradient of the volume A. The method is iterative as the demons may work until during a specific number of step or until a matching criteria is reached. The implementation we use is based on Thirion's algorithm <a name='Thirion:1998hg' class='ref'></a> (itk::DemonsRegistrationFilter).
	</p>

	<p>
		This method takes as inputs two volumes A and B and compute a new volume whose voxel are 3D scalar vectors representing the local voxel motion to morph B into A. Figure XXXX shows the superposition of the isosurfaces of two consecutive time step's volume at the marginal region of the Zebrafish embryo. The motion of the image is superposed on figure XXXX. The deformation field is computed for the all image A and B, not only in the relevant part of the images. Thus, we perform a filtering of the deformation field based on their position. The vectors belonging to voxels which are inside the nuclei envelopes (determined by an isosurface thresholding at two times the averaged volume intensity) are kept and the others are set to null vector. We obtain the deformation fields for each angle of view independently.
	</p>

	<p>
		The next module deals with the blending of these deformation fields.
	</p>






	<figure id='washington_vueGlobal_frame'>
		<img src="../../images/Reconstruction/washington/deffield/vueGlobal_frame.png" width="900">
		<figcaption></figcaption>
	</figure>



	<figure id='washington_vue1ts'>
		<img src="../../images/Reconstruction/washington/deffield/vue1ts.png" width="900">
		<figcaption></figcaption>
	</figure>


	<figure id='washington_vue2ts'>
		<img src="../../images/Reconstruction/washington/deffield/vue2ts.png" width="900">
		<figcaption></figcaption>
	</figure>


	<figure id='washington_vueDeffield2tsNoFiltering'>
		<img src="../../images/Reconstruction/washington/deffield/vueDeffield2tsNoFiltering.png" width="900">
		<figcaption></figcaption>
	</figure>


	<figure id='washington_vueDeffield2ts'>
		<img src="../../images/Reconstruction/washington/deffield/vueDeffield2ts.png" width="900">
		<figcaption></figcaption>
	</figure>





 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='6_2_2_6_'>
<h2>Blended deformation fields</h2>
 





	<figure id='washington_workflow_blend_deffield'>
		<img src="../../images/Reconstruction/washington/workflow_blend_deffield.png" width="600">
		<figcaption></figcaption>
	</figure>


	<p>
		The blending function is applied on the deformation vector field obtained in the previos section. Each voxel of the reference volume has the deformation vector from the angle of view which has the larger quality evaluation score.
	</p>




	<figure id='washington_blendingfunction_deffield'>
		<img src="../../images/Reconstruction/washington/blendingfunction_deffield.png" width="900">
		<figcaption></figcaption>
	</figure>


	<figure id='washington_blendingfunction_deffield_multits'>
		<img src="../../images/Reconstruction/washington/blendingfunction_deffield_multits.png" width="900">
		<figcaption></figcaption>
	</figure>



 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
