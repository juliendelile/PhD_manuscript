
 
 
 
 
<div id='1_'>
<h1>1. Introduction  </h1>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='1_1_'>
<h2>1.1. Context  </h2>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='1_1_1_'>
<h3>1.1.1. new data - new model  </h3>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='1_1_2_'>
<h3>1.1.2. new workflow  </h3>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='1_2_'>
<h2>1.2. Objectives  </h2>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='1_2_1_'>
<h3>1.2.1. design a integrative platform for embryonic development modeling  </h3>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='1_2_2_'>
<h3>1.2.2. validate the model in the case of zebrafish early gastrulation  </h3>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='1_3_'>
<h2>1.3. Methodology  </h2>
 


problematique : proposer un nouveau cadre theorique

ce cadre est integratif, il ne propose pas de nouveau principe theorique

mais il est integré dans la reconstruction pour permettre diagramme de phase (approche physicienne)




 
<p>
	Goal of this section : describe how theoretical and more precisely, simulated theoretical hypotheses is integrated in the process of experimental science. 
</p>


<figure id='experimental_science_schematic'>
	<img src="../../images/experimental_science/experimental_science_raw_v2.png" width="950">
	<figcaption></figcaption>
</figure>

Experimental Science augmented with Computer Simulation. Simulated seashell extracted from Fowler, D.R., Meinhardt, H. &amp; Prusinkiewicz, P., 1992. Modeling seashells. Proceedings of SIGGRAPH '92. Fowlery:1992wb


2 actors : individual and reality / environment

3 process: perception, generation of hypothesis (modeling), experimentation

<p>
	Experimental science involves an individual and its environment (&quot;Reality&quot;). This activity shares similarities with other of its explanation-seeking activities. 

	Experimental science is characterized by three fundamental processes: the perception of its environment, the generation of new hypotheses and the experimentation on the environment (see figure <a name='experimental_science_schematic' class='fig'></a>). 

</p>




<p>
	1. The loop can be entered by the individual perceiving his environment. 
</p>


<p>
	2. The observations are then matched with the knowledge of the individual. Most of the time, if the observation conforms to the knowledge, no reactive signal is emitted. However, a significant difference would trigger a signal of curiosity which will challenge the existing set of hypotheses of the individual and lead him to reconsider some of them.
</p>

<p>
	3. The process of how the individual create new hypotheses will not be discussed in this manuscript (analogy, inference, induction, abduction, deduction...). What will be discussed is the conceptual nature of these hypotheses (see below).
</p>

<p>
	4. The &quot;experimental&quot; qualification of sciences like biology or physics is due to their ability to couple the &quot;pure thinking&quot; exercise of generating new hypotheses with interaction on the &quot;real&quot; system in order to test the validity of these new hypotheses. 
</p>



<p>
	The environment of the individual is made of multiple potential object of study. 

	The specification of the object of study induces a separation of the object from its own environment which may, or may not, include the individual. In developmental biology, the studied object is the embryo and the individual is excluded from the embryo's environment.
</p>









<p><strong>Exchange/Validation by the scientific community</strong></p>


<p>
	Even if the individual is at the center of experimental science, experimental science is a collective effort. The interaction with the scientific community operates bidirectionally.
</p>

<p>
	All the hypotheses the individual makes are build upon an accumulation of prior scientific works. He can access nearly all knowledge produced by the scientific community thanks to conferences or the various scientific papers databases (Pubmed, arXiv.org, IEEE, ACM, Google Scholar...).
</p>

<p>
	One particularity of science is that validation is made by the approval of the community of scientists. Through the peer-reviewed publication system, each new work is filtered before availability by a panel of individual representing the community. We may distinguish two kind of validation: the validation of the scientific work containing all or some parts of the elements mentionned in figure <a name='experimental_science_schematic' class='fig'></a>, and the validation of the hypotheses contained in the scientific work itself. We will develop the latter in the following (XXXXXX  see part III).
</p>



<figure id='experimental_science_schematic_communauty'>
	<img src="../../images/experimental_science/experimental_science_communauty.png" width="950">
	<figcaption>Experimental science is a collective effort. Each member of the scientific community may send or receive scientific work. </figcaption>
</figure>













<p>
	<strong>Methodology augmented by tools</strong>
</p>

<p>

	Experimental science insists on the confrontation of the hypotheses, their consequences and the observation.

	This confrontation is improved by the means of tools. 

	Tools can be considered as the third actor in experimental science, in addition to the individual and the object of study.

</p>


recursive process

<p>
	They are also object of study by themselves. In developmental biology, the tools used to observe or perturb are themselves the focus of intensive ongoing research, from other fields of science. 
</p>

<p>
	The advances of our understanding are controlled by the advances of theses tools. Always improving microscopy increase the spatio-temporal resolution of the observations and every new microscope triggers a boom of conceptualization of these observations.
</p>

<p>
	The use of new physical tools drives methodological renewals. We will present an augmented version of the figure <a name='experimental_science_schematic' class='fig'></a> which introduces the methodology developed for this project. We distinguish the tools designed to augment the three fundamental processes of experimental science by the inverse order of usage in this project: tools to manipulate, tools to perceive and tools to conceive.
</p>



<p>
	<strong>Tools to manipulate</strong>
</p>

<p>
	These tools are designed to modify the &quot;natural&quot; behavior of the object of study, or its environment, in a controlled manner. 
	These experiments are artificial construction allowing to discriminate the hypotheses ruling the behavior of the object of study. 
	In developmental biology, the embryo can be perturb either genetically or mechanically.
	Genetic experiments comprise embryo expressing abnormal phenotype either by random mutagenesis or by morpholino injection (knocking down of specific gene). 
	Mechanical experiments can be either lesion applied to some specific tissue to study their evolution (see laser ablation between individual cell-cell boundaries <a name='FernandezGonzalez:2009hp' class='ref'></a>, or tissue dissection by laser XXXXXXXXX find ref) or mechanical constraint to measure the response of the tissue.
	Mechanotranduction mechanism allow to conceive experiments at the border between genetics and mechanics (see genetic regulation by exerting a force with magnetic tweezer and magnetic nanoparticles <a name='Desprat:2008ei' class='ref'></a>).
</p>


<p>
	<strong>Tools to perceive</strong>
</p>


<p>
	Tools can improve the perception of the object of study, from the interfacing between the real system and the observed data, to the reconstruction of these data which extracts salient features in the observation.
</p>

<p>
	Perception-oriented tools are object whose aim is to produce measures of the object of study. Measures are quantification of the physical quantity of the object of study with ordinary real numbers. It may noted that if these tools add a significant objectivity in the measurement process, allowing interpretation free comparison between measures, the physical quantity <i>per se</i> are determined by the current knowledge and thereby are not are subject to interpretation.
</p>

<p>
	An improvement of the perception-oriented tools is the widening of the nature and scales of the physical quantity measured, accessing dimensions out of reach of the individual senses. 
	In developmental biology, optical devices allowed to reach the sub-cellular scale, sending photons to camera sensors with high spatial resolution. 
</p>

<p>
	Sometimes, perception-oriented tools must be coupled with experimentation to allow measurement. For example, the zebrafish, which has been selected in part for its transparent characteristic at early stages, is modified by injecting fluorescent-protein coding RNA to highlight some structure of the cells such as membranes, or nuclei when exposed to laser stimulation.
</p>

<p>
	The widening of scales induces the increasing of the size of generated data. This tendency is amplified by the coupling of measurement tools with automated recording with computers. When an embryo is measured under microscope device, the size of the data recorded is enormous. For a few hours of development, this measure, which is the spatio-temporally discriminated quantity of light emitted by a laser-excited fluorescent zebrafish, is composed of billions of values (for example, 200 3D-volume of voxels of light quantity obtained each 3 minutes, each volume having a resolution of 512x512x200, gives 10.48576 billions of values). This data size may also be multiply by the number of channel used for excitation (for example if both the cell nuclei and the cell membranes are captured). We call this microscope output data the 4D (3D + time) raw data set (or <i>raw data</i> for simplicity's sake). 
</p>

reconstruction needed

<p>
	This kind of extremely large data is not directly accessible by the individual. He can not gain biological insight from these raw data. Processing of the data is needed to allow interpretation and comparison with hypotheses. We call this processing <i>reconstruction</i> of the data. The reconstruction is a series of subprocesses organized as a <i>workflow</i>. Each subprocess does a specific task which extract some information from the input data sets and generate a new data set. Computers propose visualization software tools which allows the individual to create 2D movies of the captured developmental sequence. However, if a movie permits qualitative insights, it looses the quantitative measurement which would be used for comparison. A reconstruction which is useful for developmental biology question is extraction from the raw data of the lineage tree of captured cells. For a given cell at a given time step, this data set stores the identification number of the same cell at the previous time step. With the lineage tree, cells can be followed through time and, as cell divides during the embryo's development, and form diverging branches of the &quot;tree&quot;. In addition, complementary information can be stored for each cell at each time step: the 3D coordinates, the lists of neighbors, the quantity of captured protein signal, or ligand, or any fluorescent labeled molecules... The final data sets of the workflow compose the <i>reconstructed embryo</i>. It stores all the biologically relevant information contained in the raw data set at the cellular scale. 
</p>


<p>
	Some parts of the aforementioned reconstruction workflow can be realized with commercial softwares. However, the high number of cells involved and the difficulty to interpret and manipulate the 3D volume of data initiated the design of adapted softwares and the automation of most of these subprocesses. These tasks are the past and current results of the European project Bioemergences which developed a generic workflow of reconstruction of the lineage tree in vertebrate embryos. New specific modules has been developed for this project. The detailed presentation of the reconstruction workflow is done in chapter XXXXXXX.
</p>




<figure id='experimental_science_perception_augmented'>
	<img src="../../images/experimental_science/experimental_science_perception_augmented.png" width="950">
	<figcaption>Augmentation of the perception by a reconstruction workflow.</figcaption>
</figure>






validation of the reconstruction ?? (superflu ici ?)












	









outils pour concevoir

rapprocher concevoir de percevoir


<p>
	<strong>Tools to conceive</strong>
</p>


part a: general definition

<p>
	Models or hypotheses aim at describing the whole picture of the studied phenomenon. They withdraw some supposed details and generalize the underlying mechanisms. They establish relations between the observed data.
</p>


means of expression constraints the models

<p>
	Models are constraint by their means of expression. The description of the structures and their interactions varies whether their are expressed through the verbal language, the graphical language or the mathematical language. There exists links between all these means of expression and models are often described through a combination of some of them.  
</p>

are we creating a new language for expressing ideas with computer simulation, interactive graphical language ?


classical/theoretical dichotomy

<p>
	In the context of developmental biology, a distinction is often expressed between &quot;classical&quot; studies and theoretical studies. 

	The former studies often expose &quot;qualitative&quot; models: models highlighting the nature of interaction between elements with no quantitative parameters. 

	The latter studies use the mathematical formalism to express the hypotheses. The elements are represented by variable and their interaction is formulated through equations fine-tuned by parameters. 

	The gain of theoretical studies is an increase of predicability of the hypotheses. 

	The consequence can be tested extensively, allowing to automatically reverse engineer the studied phenomenon. 

	This comes at the cost of a "simplification and an idealization, and consequently a falsification" (Turing <a name='Turing:1952vn' class='ref'></a>) of the description of the elements, which may perplexed "classical" biologists who are used to deal with the complexity of living systems. 
</p>

<p>
	Either theoretical or &quot;classical&quot;, the role played by model remains invariant. However, theoretical models bring some advantages: 
</p>


<ul>
	<li>their formalized nature enables the design of precise experimental measures.</li>
	<li>their predictability allow to test new hypothesis by examining the consequences of unobserved phenomenon. </li>
	<li>they allow to integrate a wide range of observation. Without the help of formalization, the consequence of multiple interaction quickly become unpredictable by &quot;pure though&quot; experiment only. </li>
</ul>




specific vs general question answering

<p>
	To deepen the previous advantage, we may provide cognitive scientist Marvin Minsky's definition of model in his 1965's text &quot;Matter, mind and models&quot; <a name='Minsky:1965wb' class='ref'></a>:
</p>

<p>
	&quot;To an observer B, an object A* is a model of an object A to the extent that B can use A* to answer questions that interest him about A&quot;.
</p>

<p>
	This definition is centered around the notion of the question asked by the observer (or the individual in this text). It implies that any attempt of model type categorization should start by categorizing question type. Indeed, a distinction can be made between specific and general questions. In fundamental physics, the distinction is not so clear, as each specific question as generally massive repercussion on everything else. However, in biology, the gap is larger. As an example, we may cite the specific model which studies the shape of cells in a epithelium by Gibson et al. <a name='Gibson:2006gia' class='ref'></a>, as opposed to generic models aiming at simulating various developmental phenomena like Compucell <a name='Izaguirre:2004vr' class='ref'></a> <a name='Cickovski:2007tj' class='ref'></a> or Cellerator <a name='Shapiro:2001wu' class='ref'></a> <a name='Shapiro:2003je' class='ref'></a>. The former class asks a question then designs a model to answer it whereas the latter builds an integrative model before answering various potential questions. The latter class of models is proper to the theoretical category and, as we may show in the following, it is even part of a subcategory of theoretical models.
</p>




dire que l'on presentera la plateforme de simulation dans les chapitres 3 4 5 avant une revue des connaissances actuelles du early zebrafish development et les questions que nous étudieront dans le chapitre Case Studies






Theoretical models, analytical vs computer-simulated model


<p>
	As mentioned above, theoretical models formalize interactions with equation linking some selected variable of the studied phenomenon. The solving of these <i>analytical</i> formalization is not always doable because of constraint proper to mathematics. But computer is a tool that can help in the resolution of these equations, by converting them into algorithms. These <i>numerical</i> solutions, which are approximations of the idealistic solutions, are nowadays used in most fields of research or engineering. They allow scientists to tackle more complex phenomena observed. In 1952, Alan Turing already envisioned the use of computer to help him solve more realistic reaction-diffusion pattern in &quot;The chemical basis of morphogenesis&quot; <a name='Turing:1952vn' class='ref'></a>:
</p>

<p>
	&quot;Most of an organism, most of the time, is developing from one pattern into another, rather than from homogeneity into a pattern. One would like to be able to follow this more general process mathematically also. The difficulties are, however, such one cannot hope to have any very embracing theory of such process, beyond the statement of the equations. It might be possible, however, to treat a few particular cases in detail with the aid of a digital computer. This method has the advantage that it is not so necessary to make simplifying assumption as it is when doing a more theoretical type of analysis. &quot;
</p>


<p>
	Turing emphasizes the fact that the use of computer simulation is not only a practical solution for going over analytically unsolvable mathematical equation but also that it allows the individual to integrate mechanisms that he would refrain from using because of the unsolvability. In this sense, the computer (as a Turing machine) is a tool that augment the ability of the individual to develop mathematical models of the object of study. 
</p>


<figure id='experimental_science_hypotheses_augmented'>
	<img src="../../images/experimental_science/experimental_science_hypotheses_augmented.png" width="950">
	<figcaption></figcaption>
</figure>



<p>
	In particular, a category of analytically unsolvable model is called the <i>many-boly problem</i>. It occurs when a large number of elements are interacting together. As we will present in section (XXXXXXX check), the physical approach we have chosen for our embryogenesis model is based on this assumption, each cell being an elementary particle interacting with its neighbors. Solving this system of equations is highly computationally intensive and requires the use of computers. Computers were originally invented to deal with theses situations (for example, the MonteCarlo simulation performed on the MANIAC computers in the early 1950s <a name='Metropolis:1987ws' class='ref'></a>). 
</p>


<p>
	Figure <a name='experimental_science_hypotheses_augmented' class='fig'></a> illustrates the process of transformation of the model, from the original hypotheses made by the individual, to its theoretical form and finally to its final form as a computer program.
</p>










role du modele computationel, montrer qu'une hypothese permet d'obtenir un comportement (impossible à concevoir par le seul exercice de pensée). Ces hypotheses sont suffisantes pour produire les observations.



rule out statistical models...

<p>
	A different kind of theoretical models are the statistical models. We rule out this category as they do not follow the individual-induced hypotheses framework. These models offer ways to simulate observed data without including <i>a priori</i> knowledge about these data. They possess predictive capacity but no explanatory value as the model produced is always a <i>black box</i> for the individual.
</p>






<p>
	ajouter à la section tools to conceive
</p>


methodology 

modeling allow to remove the Spemann and Gluecksohn-schoenheimer dichotomy (see timeline):

either modify the inputs and see how the phenotype behaves (Spemann)

either observe phenotype (cluster) and relate it back to the inputs (Gluecksohn)


<figure id='experimental_science_phase_diagram'>
	<img src="../../images/experimental_science/phase_diagram.png" width="900">
	<figcaption></figcaption>
</figure>



<figure id='auieauieauie'>
	<img src="../../images/experimental_science/PSExplorer_method.png" width="9500">
	<figcaption></figcaption>
</figure>







<p>
	<strong>Validation of the hypotheses</strong>
</p>


no true model

<p>
	The term <i>validation</i> of an hypotheses employed in this section can not be understood in the sense of stating that an hypothesis is true. 

	Oreskes et al. has demonstrated that establishing the truth of a proposition is possible only in a closed system and that models that used incompletely known input parameters as are models in developmental biology are never closed systems <a name='Oreskes:1994gn' class='ref'></a>. 

	Popper also advocates that one cannot prove theories and laws and that they can only be falsified <a name='Popper:1959uo' class='ref'></a>.

	The acceptation of the term <i>validation</i> must mean <i>consistency</i> between the output of the model and the observation of the object of study. The observations do support the probability of the model <a name='Oreskes:1994gn' class='ref'></a>, or its empirical adequacy <a name='vanFraassen:1980uy' class='ref'></a>.

</p>







plus on le confronte aux observations, plus il est acceptable

<p>
	The more observed data are positively confronted to the model, the more adequate it becomes. The diversity of the observed data is also a factor favoring the adequacy of the model.

	The observed reconstructed data evoked in the previous section are large data sets of various type.

	The strategy we adopt is to integrate the simulation platform and the reconstruction workflow. 

	It aims at evaluating the adequacy of the model with the assistance of all tools mentioned above. 

	This process of evaluation is equivalent to establishing the fitness of the data generated by the simulation and the observed data, by the means of <i>fitness function</i>.

</p>


<p>
	We distinguish two types of fitness function in addition to the original cognitive fitness represented by the symbol \(\Delta\) in figure <a name='experimental_science_larger_augmented' class='fig'></a>:
</p>

<ul>
	<li>
		the automated fitness function category, denoted by the symbol \(\Delta_a\).  

		These functions require a reconstruction strategy from the data generated by the simulation platform similar to the reconstruction workflow described in the augmented perception part. 

		The simulated raw data are of a different kind than the experimental raw data. The first step of the new reconstruction workflow is to perform transformation of the simulated raw data to match the format of the reconstructed experimental data.

		This process is represented on figure <a name='experimental_science_larger_augmented' class='fig'></a>, the green dashed line represents the stage on both reconstruction workflow were both reconstructed data are of the same kind. 

		Once data format matches, it becomes possible to design automated <i>fitness function</i> to evaluate the discrepancy between both reconstructed data sets and give a quantitative score. 

	</li>
	<li>the visual fitness function category, denoted by the symbol \(\Delta_v\). 

		This category exists because of the visual aspect of the data. Each data can be visualized and the individual may intuitively dismissed some hypotheses based on the visualization only.
	</li>
</ul>



<p>
	An orthogonal dichotomy distinguish the fitness function whether the observed data which are matched with the simulated data are originated from:
</p>


	 
<ul>
	<li>
		the reconstruction of experimental raw data. These fitness function are called <i>experimental reconstruction fitness</i> (ERF) functions.
	</li>
	<li>
		theoretical data representing idealized phenotypic behavior. We denote theses fitness function by <i>theoretical fitness</i> (TF) functions.
	</li>
</ul>






<figure id='experimental_science_larger_augmented'>
	<img src="../../images/experimental_science/experimental_science_larger_augmented.png" width="950">
	<figcaption></figcaption>
</figure>






on peut dire qu'il est plus probable mais on ne peut pas evaluer la probabitilé


However, situation of l'on montre qu'un modèle est meilleur qu'un autre car il est plus proche des observations










<p>
	The automated hypotheses evaluation strategies will be exploited in section 9 (XXXXXXchek) through a series of case studies. The general run of the strategy is to exhaustively map the parameter space with a fitness value in order to :

</p>

<ul>
	<li>assert the probability of the model to show that the hypotheses are sufficient to reproduce the observation in satisfying manner.</li>
	<li>discuss the topology of the map to find different mode of exploitation of the hypotheses.</li>
</ul>





experimental science, 2 levels of validation. 1. reproduce the observed data. 2. perturb (experimentally) the system in both reality and model and check if the match is still OK (if it does, we suppose that the model yields more generalized rules). 




two goals in mind: 
1 assert the probability of the model to show that the hypo are sufficient to reproduce the observation, 
2 suppose the model is valid, and reverse engineer the parameters


Les hypotheses du modele sont suffisantes pour produire les observations. dans la partie 1 de la validation...


	Moreover, if proving the necessity of the hypotheses is not possible, their sufficiency can at least be demonstrated. 

	This allows the computational testing of new hypotheses before further discriminating studies. 

...end







(en option) issue of data selection. if the model does not fit, change the observed data !!!






<p>
	ancien schéma 1
</p>



<figure id='auieauieuaie'>
	<img src="../../images/experimental_science/old/method_cartography.png" width="950">
	<figcaption></figcaption>
</figure>


<p>
	ancien schéma 2
</p>


<figure id='auiraunietnrasuite'>
	<img src="../../images/experimental_science/old/reconstruction_workflow.png" width="950">
	<figcaption></figcaption>
</figure>



<p>
	ancien schéma 3
</p>


<figure id='auieauie'>
	<img src="../../images/experimental_science/old/methodology_EB2011V2.png" width="950">
	<figcaption></figcaption>
</figure>




<p>
	nouvelle proposition (sketch)
</p>

<figure id='ansriuetnarsite'>
	<img src="../../images/experimental_science/experimental_science_cleaner.png" width="950re">
	<figcaption></figcaption>
</figure>




</div>
 














<p>
	Oberkampf, W.L. &amp; Roy, C.J., 2010. Verification and Validation in Scientific Computing 1st ed, Cambridge University Press. <a name='Oberkampf:2010uw' class='ref'></a>
</p>


<p>
	&quot;Verification is the process of assessing software correctness and numerical accuracy of the solution to a given mathematical model. Validation is the process of assessing the physical accuracy of a mathematical model based on comparisons between computational results and experimental data. Verification and validation (V&V) are the primary processes for assessing and quantifying the accuracy of computational results. The perspective of V&V is distinctly on the side of skepticism, sometimes to the degree of being radical (Tetlock, 2005). In verification, the association or relationship of the simulation to the real world is not an issue. In validation, the relationship between the mathematical model and the real world (experimental data) is the issue.&quot;
</p>


<p>
	&quot;Blottner (1990) captured the essence of each in the compact expressions: “Verification is solving the equations right;” “Validation is solving the right equations.”&quot;
</p>

<p>
	&quot;When this viewpoint is carried to the extreme (Oreskes et al., 1994), one is left with the following position: one can only disprove or fail to disprove theories and laws of nature. Stated differently, theories and laws cannot be proved; they can only be falsified (Popper, 1969)&quot;
</p>


<p>
	&quot;During the last two decades a workable and constructive approach to the concepts, terminology, and methodology of V&V has been developed, but it was based on practical realities in business and government, not the issue of absolute truth in the philosophy of nature.&quot;
</p>


<p>
	&quot;Numerical algorithm verification is fundamentally empirical. &quot;
</p>


<p>
	Model validation: 
</p>


<p>
	aspect 1: &quot;Quantification of the accuracy of the computational model results by comparing the computed system response quantities (SRQs) of interest with experimentally measured SRQs.&quot;
</p>


<p>
	aspect 2: &quot;Use of the computational model to make predictions, in the sense of interpolation or extrapolation of the model, for conditions corresponding to the model’s domain of intended use.&quot;
</p>


<p>
	aspect 3: &quot;Determination of whether the estimated accuracy of the computational model results satisfies the accuracy requirements specified for the SRQs of interest.&quot;
</p>

<p>
	&quot;The three major types of model are conceptual, mathematical, and computational.&quot;
</p>

<p>
	&quot;Prediction: use of a computational model to foretell the state of a physical system under conditions for which the computational model has not been validated.&quot;
</p>

<p>
	&quot;Calibration: the process of adjusting physical modeling parameters in the computational model to improve agreement with experimental data.&quot;
</p>


<p>
	&quot;Calibration is primarily directed toward improving the agreement of computational results with existing experimental data, not determining the accuracy of the results.&quot;
</p>

<p>
	&quot;if one examines a complete system, it is found that some elements of the validation hierarchy involved calibration and some are focused on validation.&quot;
</p>



<p>
	Oreskes, N.N., Shrader-Frechette, K.K. &amp; Belitz, K.K., 1994. Verification, validation, and confirmation of numerical models in the Earth sciences. Science, 263(5147), pp.641–646. <a name='Oreskes:1994gn' class='ref'></a>
</p>

from the laws of symbolic logic, &quot;it is impossible to demonstrate the truth of any proposition, except in a closed system.&quot; The models are never closed systems, in part because their input parameters are incompletely known (ex. hydrogeology, geochemistry), or because information is lost between the lower scale and the averaging scale (continuum mechanics), or because so unavailable data force effect to be neglected, or because assumption are approximative at some level... 


<p>
	review: Kleindorfer, G.B., O'Neill, L. &amp; Ganeshan, R., 1998. Validation in simulation: Various positions in the philosophy of science. Management Science, 44(8), pp.1087–1099. <a name='Kleindorfer:1998ww' class='ref'></a>
</p>


<p>
	Popper, 1959. The Logic of Scientific Discovery, Basic Books. <a name='Popper:1959uo' class='ref'></a>
</p>



 


this methodology applied to developmental biology is still in its infancy. Both at the reconstruction or the simalution sides.

we may not be looking for a new theory but to assemble block of existing cellular theories






 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
<div id='1_4_'>
<h2>1.4. Chapters summary  </h2>
 
 
</div>
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
